Spark2 for Debian
----------------

This package is created from the release tarballs provided from
https://spark.apache.org/downloads.html. It does not build Spark from source.

This repository was created using git-import-orig:

  SPARK_VERSION=2.3.0
  HADOOP_VERSION=2.6
  VERSION=${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
  wget -P /tmp http://ftp.wayne.edu/apache/spark/spark-$SPARK_VERSION/spark-$VERSION.tgz
  gbp import-orig -u $VERSION --upstream-branch=master --debian-branch=debian /tmp/spark-$VERSION.tgz
  debian/extra/download_extras.sh # Download any extra binaries

You should be able to import a new upstream release using the same command again.

Note the download_extras.sh command. At the time of writing (2018-08), this command
is only used to download pyarrow, a suggested library to help speed up use of Python
and Spark.  Since it downloads a binary wheel, you should make sure you are running
download_extras.sh on the same platform architecture you intend to install the
spark2 package on.

Once you've imported the Spark 2 tarball, you need to explicitly declare all
binary files that need to be included in the package. The following command should do the trick.

  find {jars,python/lib,yarn,R,debian/extra} -type f -exec file {} \; | grep -v text | awk -F ':' '{print $1}' | sort > debian/source/include-binaries

You also may need to update the versionless py4j-src.zip symlink that spark2.links creates if the
py4j version has changed. If you don't do this, it is possible that things like Jupyter Notebook
pyspark2 kernels may break.

Finally, to build --include-removal needs to be passed to dpkg-source so that any removed binaries
will be removed from the source package.  You can do so by passing --source-option="--include-removal"
to git-buildpackage or dpkg-buildpackage.  On a Wikimedia build server:

  GIT_PBUILDER_AUTOCONF=no DIST=stretch WIKIMEDIA=yes gbp buildpackage -sa -us -uc --git-builder=git-pbuilder --source-option="--include-removal"

spark2-assembly.zip
-------------------
This package builds a spark2-assembly.zip archive from all files in the
jars directory.  If an HDFS, client is avaliable, spark2-assembly.zip will be
uploaded to HDFS, and spark-defaults.conf will be configured to use it.

If you are upgrading the spark2 package, you will need to reupload the assembly .zip
file to HDFS manually:

  sudo -u spark hdfs dfs -put -f /usr/lib/spark2/spark2-assembly.zip hdfs:///user/spark/share/lib/spark2-assembly.zip
