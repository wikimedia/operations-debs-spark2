Spark2 for Debian
----------------

This package is created from the release tarballs provided from
https://spark.apache.org/downloads.html. It does not build Spark from source.

To import a new Spark distribution version, do the following:

  export SPARK_VERSION=2.4.4
  export SPARK_HADOOP_VERSION=2.6
  # Only set this if different than SPARK_HADOOP_VERSION.  If this is set,
  # The hadoop jar dependencies will be replaced with this version when preparing the build.
  export WMF_HADDOP_VERSION=2.10.1

  SPARK_DIST_VERSION=${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}
  wget -P /tmp https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_DIST_VERSION}.tgz
  gbp import-orig -u $VERSION --upstream-branch=master --debian-branch=debian /tmp/spark-${SPARK_DIST_VERSION}.tgz

  # Download extra dependencies we want to include.
  debian/extra/prepare_build.sh

You also may need to update the versionless py4j-src.zip symlink that spark2.links creates if the
py4j version has changed. If you don't do this, it is possible that things like Jupyter Notebook
pyspark2 kernels may break.

Finally, to build --include-removal needs to be passed to dpkg-source so that any removed binaries
will be removed from the source package.  You can do so by passing --source-option="--include-removal"
to git-buildpackage or dpkg-buildpackage.  On a Wikimedia build server:

  GIT_PBUILDER_AUTOCONF=no DIST=buster WIKIMEDIA=yes gbp buildpackage -sa -us -uc --git-builder=git-pbuilder --source-option="--include-removal"


Hadoop Versions
---------------

Spark 2 distributions are available with included Hadoop dependencies, as well as without.
The Spark 2 dists without Hadoop are missing some other crucial spark related things that we need,
including Hive.  We could possibly make a custom build from source.

For now, the .deb that this builds will manually replace the Hadoop dependencies in the Spark
dist with the Hadoop dependences at the version WMF runs (if WMF_HADOOP_VERSION is set).

More detail can be found in the various scripts called by prepare_build.sh.

spark-*-assembly.zip
--------------------
This package builds a spark-${SPARK_VERSION}-assembly.zip archive from all files in the
jars directory.  This should be manually installed into HDFS and the value of
spark.yarn.archive should be set in spark-defaults.conf.

spark-*-pythonX.Y.zip
---------------------
THis package builds .zip files containing python version specific dependencies
collected from the file downloaded in extras.  By default, pyspark will
include the proper dependencies found in usr/lib/spark2/pythonX.Y.  However,
if you are installing this package in a distributed environment, you may need
to support an incremental upgrade.  In this case, you can submit the appropriate
python .zip file along with a job in order to use same python dependencies you have
on your client in remote executors.  To ensure that the proper python dependencies are
shipped from your client, add
  --archives "/usr/lib/spark2/spark-${SPARK_VERSION}-pythonX.Y.zip#spark-${SPARK_VERSION}-pythonX.Y" \
  --conf "spark.executorEnv.PYTHONPATH=spark-${SPARK_VERSION}-pythonX.Y"
to your pyspark2 or spark2-submit command.
Note that we can't use --py-files here, as python cannot load compiled modules from .zip files.
The .zip file must be extracted (via --archives) and then added to the executor's PYTHONPATH.
