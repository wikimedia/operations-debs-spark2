#!/usr/bin/make -f

# default spark_version to version of spark-core .jar
SPARK_VERSION ?= $(shell ls $(CURDIR)/jars/spark-core*.jar | sed 's/.*-\([0-9\.][0-9\.]*\).jar/\1/')

%:
	dh $@

override_dh_install:
	dh_install

	# Copy libs into usr/lib/spark2/, excluding hadoop-* jars.
	# Hadoop is expected to be provided elsewhere if needed.
	find jars -not -name 'hadoop-*.jar' -exec cp -v {} $(CURDIR)/debian/spark2/usr/lib/spark2/jars/ \;

	cp -rv $(CURDIR)/R/* $(CURDIR)/debian/spark2/usr/lib/spark2/R/
	cp -rv $(CURDIR)/yarn/* 	$(CURDIR)/debian/spark2/usr/lib/spark2/yarn/
	# (Avoid copying python test files by specifying the files we want from python/ directory.)
	cp -rv $(CURDIR)/python/docs $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/lib $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/pyspark $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/MANIFEST.in $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/pylintrc $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/README.md $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/setup.cfg $(CURDIR)/debian/spark2/usr/lib/spark2/python/
	cp -rv $(CURDIR)/python/setup.py $(CURDIR)/debian/spark2/usr/lib/spark2/python/

	# Copy desired bin scripts
	cp -v $(CURDIR)/bin/beeline           $(CURDIR)/debian/spark2/usr/lib/spark2/bin/beeline
	cp -v $(CURDIR)/bin/find-spark-home   $(CURDIR)/debian/spark2/usr/lib/spark2/bin/find-spark-home
	cp -v $(CURDIR)/bin/load-spark-env.sh $(CURDIR)/debian/spark2/usr/lib/spark2/bin/load-spark-env.sh
	cp -v $(CURDIR)/bin/pyspark           $(CURDIR)/debian/spark2/usr/lib/spark2/bin/pyspark
	cp -v $(CURDIR)/bin/spark-class       $(CURDIR)/debian/spark2/usr/lib/spark2/bin/spark-class
	cp -v $(CURDIR)/bin/spark-shell       $(CURDIR)/debian/spark2/usr/lib/spark2/bin/spark-shell
	cp -v $(CURDIR)/bin/spark-sql         $(CURDIR)/debian/spark2/usr/lib/spark2/bin/spark-sql
	cp -v $(CURDIR)/bin/spark-submit      $(CURDIR)/debian/spark2/usr/lib/spark2/bin/spark-submit
	cp -v $(CURDIR)/bin/sparkR            $(CURDIR)/debian/spark2/usr/lib/spark2/bin/sparkR

	# Create a spark2-assembly.zip archive that contains everything in usr/lib/spark2/jars
	# This can be used for spark.yarn.archive to avoid having to load
	# all .jar files to HDFS everytime spark is launched.
	zip -j $(CURDIR)/debian/spark2/usr/lib/spark2/spark-$(SPARK_VERSION)-assembly.zip $(CURDIR)/debian/spark2/usr/lib/spark2/jars/*

	# Install any extra Python wheels we want to ship with spark2.
	find $(CURDIR)/debian/extra/python -name "*.whl" -exec unzip {} -d $(CURDIR)/debian/spark2/usr/lib/spark2/python \;
	find $(CURDIR)/debian/extra/python3.5 -name "*.whl" -exec unzip {} -d $(CURDIR)/debian/spark2/usr/lib/spark2/python3.5 \;
	find $(CURDIR)/debian/extra/python3.7 -name "*.whl" -exec unzip {} -d $(CURDIR)/debian/spark2/usr/lib/spark2/python3.7 \;

	# Also create 'python assembly' zip files.  These can be used when running pyspark in YARN
	# as the --archives argument.  This will allow you to run this version of pyspark
	# with expected extra dependencies in a distributed environment, without having to
	# also install this debian package everywhere.  Hopefully this will ease in upgrade
	# testing.  To use, install and launch pyspark with e.g.
	#  --master yarn --archives '/usr/lib/spark2/spark-2.4.3-python3.5.zip#spark-2.4.3-python3.5'  --conf 'spark.executorEnv.PYTHONPATH=spark-2.4.3-python3.5'
	cd $(CURDIR)/debian/spark2/usr/lib/spark2/python && zip -r $(CURDIR)/debian/spark2/usr/lib/spark2/spark-$(SPARK_VERSION)-python3.5.zip ./* && zip -r $(CURDIR)/debian/spark2/usr/lib/spark2/spark-$(SPARK_VERSION)-python3.7.zip ./*
	cd $(CURDIR)/debian/spark2/usr/lib/spark2/python3.5 && zip -r $(CURDIR)/debian/spark2/usr/lib/spark2/spark-$(SPARK_VERSION)-python3.5.zip ./*
	cd $(CURDIR)/debian/spark2/usr/lib/spark2/python3.7 && zip -r $(CURDIR)/debian/spark2/usr/lib/spark2/spark-$(SPARK_VERSION)-python3.7.zip ./*

